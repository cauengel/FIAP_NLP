{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24730.960917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14277.792868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12366.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37095.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49460.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "count  49459.000000\n",
       "mean   24730.960917\n",
       "std    14277.792868\n",
       "min        1.000000\n",
       "25%    12366.500000\n",
       "50%    24731.000000\n",
       "75%    37095.500000\n",
       "max    49460.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#https://s3.amazonaws.com/aulas-fiap/imdb-reviews-pt-br.csv\n",
    "df_original = pd.read_csv('https://s3.amazonaws.com/aulas-fiap/imdb-reviews-pt-br.csv')\n",
    "\n",
    "df_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_original.sample(5000,random_state=71)\n",
    "df = df_original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converte todas as palavras para minúsculo \n",
    "df.text_pt = df.text_pt.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas e digramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True)\n",
    "vect.fit(df.text_pt)\n",
    "text_vect = vect.transform(df.text_pt)\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7076632006864927\n"
     ]
    }
   ],
   "source": [
    "#Testa com Árvore de Decisão \n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(tree.get_params())\n",
    "\n",
    "parametros = {'criterion': ['gini','entropy'],\n",
    "              'splitter': ['random','best'],\n",
    "              'max_depth': [3,5,9,11],\n",
    "              'min_samples_split': [2,4,6,8] }\n",
    "        \n",
    "tree_opt = GridSearchCV(tree, parametros, scoring='f1_weighted')\n",
    "\n",
    "tree_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = tree_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(tree_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7800344502570297\n"
     ]
    }
   ],
   "source": [
    "#Testa com KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = neigh.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(neigh.get_params())\n",
    "\n",
    "parametros = {'n_neighbors': [3,5,7],\n",
    "              'weights': ['uniform','distance'],\n",
    "              'algorithm': ['ball_tree','kd_tree','brute'],\n",
    "               'p' : [1,2]}\n",
    "        \n",
    "neigh_opt = GridSearchCV(neigh, parametros, scoring='f1_weighted')\n",
    "\n",
    "neigh_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = neigh_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(neigh_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8737282798268138\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM \n",
    "\n",
    "## Bom F1 Score \n",
    "#### F1 Score de 87,37%\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(C=100, kernel='linear',random_state =42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = svm_clf.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8948569931479002\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 89,48% ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = svm_linear.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l1', 'random_state': 42, 'tol': 0.0001, 'verbose': 0}\n",
      "0.909718483582924\n",
      "{'cv': None, 'error_score': 'raise', 'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__loss': 'squared_hinge', 'estimator__max_iter': 1000, 'estimator__multi_class': 'ovr', 'estimator__penalty': 'l1', 'estimator__random_state': 42, 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=42, tol=0.0001,\n",
      "     verbose=0), 'fit_params': None, 'iid': True, 'n_jobs': 1, 'param_grid': {'penalty': ['l1', 'l2'], 'C': [1.0, 2.0, 4.0]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': 'warn', 'scoring': 'f1_weighted', 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "#### Com otimização 90,97% de f1 score #####\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(svm_linear.get_params())\n",
    "\n",
    "\n",
    "#parametros = {'penalty': ['l1', 'l2'],\n",
    "#              'C': [1.0,2.0,4.0],\n",
    "#              'multi_class':['ovr','crammer_singer'],\n",
    "#              'fit_intercept' :[True,False],\n",
    "#              'intercept_scaling' :[0.5,1.0,2.0]}\n",
    "\n",
    "\n",
    "parametros = {'penalty': ['l1', 'l2'],\n",
    "              'C': [1.0,2.0,4.0]}\n",
    "\n",
    "\n",
    "svm_linear_opt = GridSearchCV(svm_linear, parametros, scoring='f1_weighted')\n",
    "\n",
    "svm_linear_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = svm_linear_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(svm_linear_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(svm_clf.get_params())\n",
    "\n",
    "#parametros = {'kernel': ['linear', 'poly', 'rbf'],\n",
    "#              'C': [1.0,2.0,100.0],\n",
    "#              'degree': [2,3,4,5],\n",
    "#              'gamma': ['auto','scale'],\n",
    "#              'coef0' : [0.0,1.0,4.0],\n",
    "#              'decision_function_shape' :['ovo','ovr'],\n",
    "#              'shrinking' : [True,False]}\n",
    "\n",
    "parametros = {'kernel': ['linear', 'poly', 'rbf'],\n",
    "              'C': [90.0,100.0,120.0],\n",
    "              'degree': [3,4],\n",
    "              'decision_function_shape' :['ovo','ovr'] }\n",
    "\n",
    "\n",
    "svm_opt = GridSearchCV(svm_clf, parametros, scoring='f1_weighted')\n",
    "\n",
    "svm_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = svm_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(svm_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157078480589882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#rand_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "#            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#            min_samples_leaf=1, min_samples_split=2,\n",
    "#            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "rand_forest = RandomForestClassifier(n_estimators=200,random_state=42,max_depth=10)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rand_forest.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(rand_forest.get_params())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parametros = {\n",
    "              'max_depth': [40, None],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'n_estimators': [200, 400, 1000]}\n",
    "\n",
    "\n",
    "rand_forest_opt = GridSearchCV(rand_forest, parametros, scoring='f1_weighted')\n",
    "\n",
    "rand_forest_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rand_forest_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(rand_forest_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8570078748299506\n"
     ]
    }
   ],
   "source": [
    "#### Bom modelo também ####\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_berno = BernoulliNB()\n",
    "\n",
    "naive_berno.fit(X_train,y_train)\n",
    "\n",
    "y_prediction = naive_berno.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8616663092276419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_multi = MultinomialNB()\n",
    "\n",
    "naive_multi.fit(X_train,y_train)\n",
    "\n",
    "y_prediction = naive_multi.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def xgb_f1(y,t):\n",
    "    t = t.get_label()\n",
    "    y_bin = [1. if y_cont > 0.5 else 0. for y_cont in y] # binaryzing your output\n",
    "    return 'f1',f1_score(t,y_bin)\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=15, learning_rate=0.004,\n",
    "                            n_estimators=200,\n",
    "                            booster='gbtree',\n",
    "                            silent=True,   objective='binary:logistic',\n",
    "                            nthread=-1, gamma=0,\n",
    "                            min_child_weight=1, max_delta_step=0, subsample=0.8,\n",
    "                            colsample_bytree=0.6,\n",
    "                            base_score=0.5,\n",
    "                            seed=0, missing=None)\n",
    "\n",
    "\n",
    "#clf.fit(X_train, y_train, eval_metric=xgb_f1,\n",
    "#         eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "#         early_stopping_rounds=900)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train, eval_metric=xgb_f1,\n",
    "         eval_set=[(X_train, y_train)],\n",
    "         early_stopping_rounds=900)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "\n",
    "\n",
    "print(f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Tirando stop words utilizando o spacy \n",
    "# Gerando novamente os vetores de teste  \n",
    "pt = spacy.load('pt_core_news_sm')\n",
    "\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "\n",
    "\n",
    "stop_words_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vect_stop = TfidfVectorizer(ngram_range=(1,1), use_idf=True,stop_words=stop_words_spacy)\n",
    "vect_stop.fit(df.text_pt)\n",
    "text_vect_stop = vect_stop.transform(df.text_pt)\n",
    "\n",
    "X_train_stop,X_test_stop,y_train_stop,y_test_stop = train_test_split(\n",
    "    text_vect_stop, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7139183382583874\n"
     ]
    }
   ],
   "source": [
    "tree_stop = DecisionTreeClassifier(random_state=42)\n",
    "tree_stop.fit(X_train_stop, y_train_stop)\n",
    "\n",
    "y_prediction = tree_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8543677396298509\n"
     ]
    }
   ],
   "source": [
    "# Teste com Regressão Linear Bernoulli \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_berno_stop = BernoulliNB()\n",
    "\n",
    "naive_berno_stop.fit(X_train_stop,y_train_stop)\n",
    "\n",
    "y_prediction = naive_berno_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610960619816452\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_multi_stop = MultinomialNB()\n",
    "\n",
    "naive_multi_stop.fit(X_train_stop,y_train_stop)\n",
    "\n",
    "y_prediction = naive_multi_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8816112827428741\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### AINDA É MELHOR COM STOP WORDS 88,16% (SEM STOP WORDS) vs 88,88% (COM STOP WORDS) ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear_stop = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear_stop.fit(X_train_stop, y_train_stop)\n",
    "\n",
    "y_prediction = svm_linear_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(naive_multi.get_params())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parametros = {\n",
    "              'alpha': [1.0,2.0,4.0],\n",
    "              'fit_prior': [True, False]}\n",
    "\n",
    "\n",
    "naive_multi_opt = GridSearchCV(naive_multi, parametros, scoring='f1_weighted')\n",
    "\n",
    "naive_multi_opt.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = naive_multi_opt.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "print(f1)\n",
    "print(naive_multi_opt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nlp_fiap\\Anaconda3_36\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38577\n",
      "38577\n",
      "0.882915385469413\n",
      "38577\n",
      "38577\n",
      "0.8818185435656671\n",
      "38577\n",
      "38577\n",
      "0.8737887985575191\n",
      "38577\n",
      "38577\n",
      "0.8818224023777613\n",
      "38577\n",
      "38577\n",
      "0.8969747431665157\n",
      "38577\n",
      "38577\n",
      "0.8919473204113814\n",
      "38577\n",
      "38577\n",
      "0.8878827919994146\n",
      "38578\n",
      "38578\n",
      "0.886754760385754\n",
      "38578\n",
      "38578\n",
      "0.8938397524530379\n",
      "38578\n",
      "38578\n",
      "0.885761167664981\n",
      "38578\n",
      "38578\n",
      "0.8867751422539878\n",
      "38578\n",
      "38578\n",
      "0.8746439074954866\n",
      "38578\n",
      "38578\n",
      "0.8928714637599326\n",
      "38578\n",
      "38578\n",
      "0.8787457620447278\n",
      "38578\n",
      "38578\n",
      "0.8715911381208237\n",
      "38578\n",
      "38578\n",
      "0.8827203616054721\n",
      "38578\n",
      "38578\n",
      "0.8908327389204669\n",
      "38578\n",
      "38578\n",
      "0.8807336929115285\n",
      "38578\n",
      "38578\n",
      "0.878672017690485\n",
      "38578\n",
      "38578\n",
      "0.8786814476270487\n",
      "38578\n",
      "38578\n",
      "0.897878522481455\n",
      "38578\n",
      "38578\n",
      "0.8736097067745198\n",
      "38578\n",
      "38578\n",
      "0.878687652414987\n",
      "38578\n",
      "38578\n",
      "0.8786740023506429\n",
      "38578\n",
      "38578\n",
      "0.8766566954920549\n",
      "38578\n",
      "38578\n",
      "0.8807166033165168\n",
      "38578\n",
      "38578\n",
      "0.887834084634337\n",
      "38578\n",
      "38578\n",
      "0.8857494830349681\n",
      "38578\n",
      "38578\n",
      "0.8877674851033813\n",
      "38578\n",
      "38578\n",
      "0.8787335889361781\n",
      "38578\n",
      "38578\n",
      "0.8816991693389659\n",
      "38578\n",
      "38578\n",
      "0.8978808196548057\n",
      "38578\n",
      "38578\n",
      "0.8756752001398304\n",
      "38578\n",
      "38578\n",
      "0.8978879207649519\n",
      "38578\n",
      "38578\n",
      "0.885761167664981\n",
      "38578\n",
      "38578\n",
      "0.8736097067745198\n",
      "38578\n",
      "38578\n",
      "0.884732052578362\n",
      "38578\n",
      "38578\n",
      "0.8817117497970236\n",
      "38578\n",
      "38578\n",
      "0.8968845045527389\n",
      "38578\n",
      "38578\n",
      "0.8778029913001246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nlp_fiap\\Anaconda3_36\\lib\\site-packages\\ipykernel_launcher.py:40: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888486109507048\n"
     ]
    }
   ],
   "source": [
    "# Teste do melhor método com K-FOLD \n",
    "X_kfold = X_train\n",
    "Y_kfold = y_train.as_matrix()\n",
    "    \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "clf = svm_linear\n",
    "\n",
    "\n",
    "best_model = None \n",
    "best_f1 = -1 \n",
    "\n",
    "for train_index, test_index in kf.split(X_kfold,Y_kfold):  \n",
    "    X_train_kfold, X_test_kfold = X_kfold[train_index], X_kfold[test_index]\n",
    "    y_train_kfold, y_test_kfold = Y_kfold[train_index], Y_kfold[test_index]\n",
    "    \n",
    "    print(X_train_kfold.shape[0])\n",
    "    print(y_train_kfold.shape[0])\n",
    "    clf.fit(X_train_kfold, y_train_kfold)\n",
    "    y_prediction = clf.predict(X_test_kfold)\n",
    "    f1 = f1_score(y_prediction, y_test_kfold, average='weighted')\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "       best_f1 = f1\n",
    "       best_model = deepcopy(clf)\n",
    "        \n",
    "    print(f1)\n",
    "\n",
    "\n",
    "\n",
    "X_final_test = X_test \n",
    "Y_final_test = y_test.as_matrix()\n",
    "\n",
    "y_pred = best_model.predict(X_final_test)\n",
    "\n",
    "f1 = f1_score(y_pred,Y_final_test,average='weighted')\n",
    "\n",
    "\n",
    "print(f1)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\nlp_fiap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "def conv_stem(texto):\n",
    "  return ' '.join([rslp.stem(token) for token in texto.split(' ')])\n",
    "\n",
    "df['stemizado'] = df.text_pt.apply(conv_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mais uma vez, o sr. costn arrum um film por mu...\n",
       "1    est é um exempl do motiv pel qual a maior do f...\n",
       "2    prim de tud eu odei ess rap imbecis, que não p...\n",
       "3    nem mesm os beatl pud escrev músic que tod gos...\n",
       "4    film de fot de lat não é uma palavr apropri pa...\n",
       "Name: stemizado, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stemizado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.stemizado)\n",
    "text_vect = vect.transform(df.stemizado)\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_stem,X_test_stem,y_train_stem,y_test_stem = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776683709158611\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 88,88% ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear_stem = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear_stem.fit(X_train_stem, y_train_stem)\n",
    "\n",
    "y_prediction = svm_linear_stem.predict(X_test_stem)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem, average='weighted')\n",
    "\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8546256405067771\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_multi_stem = MultinomialNB()\n",
    "\n",
    "naive_multi_stem.fit(X_train_stem,y_train_stem)\n",
    "\n",
    "y_prediction = naive_multi_stem.predict(X_test_stem)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def conv_stem(texto):\n",
    "  return ' '.join([ps.stem(token) for token in texto.split(' ')])\n",
    "\n",
    "df['stemizado2'] = df.text_pt.apply(conv_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.stemizado2)\n",
    "text_vect = vect.transform(df.stemizado2)\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_stem2,X_test_stem2,y_train_stem2,y_test_stem2 = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8850488324845899\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 88,88% ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear_stem2 = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear_stem2.fit(X_train_stem2, y_train_stem2)\n",
    "\n",
    "y_prediction = svm_linear_stem2.predict(X_test_stem2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem2, average='weighted')\n",
    "\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583350796011883\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_multi_stem2 = MultinomialNB()\n",
    "\n",
    "naive_multi_stem2.fit(X_train_stem2,y_train_stem2)\n",
    "\n",
    "y_prediction = naive_multi_stem2.predict(X_test_stem2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem2, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Acrescentando informação da análise sintática \n",
    "# Gerando novamente os vetores de teste  \n",
    "pt = spacy.load('pt_core_news_sm')\n",
    "\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "def conv_sintatico(texto):\n",
    "    doc = pt(texto)\n",
    "    str = ''\n",
    "    for token in doc:\n",
    "        str += token.text + '-' + token.pos_ + ' '\n",
    "    return str \n",
    "\n",
    "df['sintatico'] = df.text_pt.apply(conv_sintatico)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.sintatico)\n",
    "text_vect = vect.transform(df.sintatico)\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_sint,X_test_sint,y_train_sint,y_test_sint = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88848613678332\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 88,88% ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear_sint = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear_sint.fit(X_train_sint, y_train_sint)\n",
    "\n",
    "y_prediction = svm_linear_sint.predict(X_test_sint)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_sint, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623891052040696\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_multi_sint = MultinomialNB()\n",
    "\n",
    "naive_multi_sint.fit(X_train_sint,y_train_sint)\n",
    "\n",
    "y_prediction = naive_multi_sint.predict(X_test_sint)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_sint, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True)\n",
    "vect.fit(df.sintatico)\n",
    "text_vect = vect.transform(df.sintatico)\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_sint2,X_test_sint2,y_train_sint2,y_test_sint2 = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8918224657761997\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 89,18% ####\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "svm_linear_sint2 = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "svm_linear_sint2.fit(X_train_sint2, y_train_sint2)\n",
    "\n",
    "y_prediction = svm_linear_sint2.predict(X_test_sint2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_sint2, average='weighted')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec Model: 929MB [02:23, 6.49MB/s]                                                                                \n"
     ]
    }
   ],
   "source": [
    "#Faz download da base \n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "tar_gz_path = './cbow_s300.zip'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "  last_block = 0\n",
    "\n",
    "  def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "    self.total = total_size\n",
    "    self.update((block_num - self.last_block) * block_size)\n",
    "    self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "  with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Word2Vec Model') as pbar:\n",
    "    urlretrieve(\n",
    "      'http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s300.zip',\n",
    "      tar_gz_path,\n",
    "      pbar.hook)\n",
    "\n",
    "if not isfile('./cbow_s300.txt'):     \n",
    "  zip_ref = zipfile.ZipFile(tar_gz_path, 'r')\n",
    "  zip_ref.extractall('./')\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_cbow = KeyedVectors.load_word2vec_format('./cbow_s300.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' esse é um teste de  tentativa'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def conv_word2vec_frase(frase):\n",
    "    soma =  0 \n",
    "    for palavra in frase.split(' '):\n",
    "        palavra = palavra.translate(palavra.maketrans('', '', string.punctuation))\n",
    "        try:\n",
    "            soma = soma + mod \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'mais uma vez, o sr. costner arrumou um filme por muito mais tempo do que o necessário. além das terríveis seqüências de resgate no mar, das quais há muito poucas, eu simplesmente não me importei com nenhum dos personagens. a maioria de nós tem fantasmas no armário, e o personagem costers é realizado logo no início, e depois esquecido até muito mais tarde, quando eu não me importava. o personagem com o qual deveríamos nos importar é muito arrogante e superconfiante, ashton kutcher. o problema é que ele sai como um garoto que pensa que é melhor do que qualquer outra pessoa ao seu redor e não mostra sinais de um armário desordenado. seu único obstáculo parece estar vencendo costner. finalmente, quando estamos bem além do meio do caminho, costner nos conta sobre os fantasmas dos kutchers. somos informados de por que kutcher é levado a ser o melhor sem pressentimentos ou presságios anteriores. nenhuma mágica aqui, era tudo que eu podia fazer para não desligar uma hora.' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a8965d749511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_pt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" ,\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3_36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'mais uma vez, o sr. costner arrumou um filme por muito mais tempo do que o necessário. além das terríveis seqüências de resgate no mar, das quais há muito poucas, eu simplesmente não me importei com nenhum dos personagens. a maioria de nós tem fantasmas no armário, e o personagem costers é realizado logo no início, e depois esquecido até muito mais tarde, quando eu não me importava. o personagem com o qual deveríamos nos importar é muito arrogante e superconfiante, ashton kutcher. o problema é que ele sai como um garoto que pensa que é melhor do que qualquer outra pessoa ao seu redor e não mostra sinais de um armário desordenado. seu único obstáculo parece estar vencendo costner. finalmente, quando estamos bem além do meio do caminho, costner nos conta sobre os fantasmas dos kutchers. somos informados de por que kutcher é levado a ser o melhor sem pressentimentos ou presságios anteriores. nenhuma mágica aqui, era tudo que eu podia fazer para não desligar uma hora.' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for frase in df.text_pt:\n",
    "    for palavra in frase.split(\" \"):\n",
    "        palavra = \n",
    "        print(model[palavra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.11400e-03, -6.14320e-02,  3.67100e-02,  3.58087e-01,\n",
       "        4.17990e-02,  1.04200e-02,  1.75388e-01,  8.31050e-02,\n",
       "       -2.18300e-02,  1.04309e-01,  4.75109e-01, -7.24770e-02,\n",
       "        1.23979e-01,  1.61774e-01,  7.73480e-02, -3.32580e-02,\n",
       "        1.34390e-01, -2.12611e-01, -2.06812e-01, -1.49509e-01,\n",
       "        1.78139e-01, -3.79871e-01, -1.69589e-01, -7.77560e-02,\n",
       "        2.09633e-01,  1.18871e-01,  1.15192e-01,  1.95392e-01,\n",
       "        3.28171e-01,  3.24069e-01, -3.49784e-01, -2.49682e-01,\n",
       "        1.83095e-01, -7.85330e-02,  1.02388e-01, -1.56267e-01,\n",
       "        2.64185e-01, -3.50361e-01, -7.69280e-02,  1.02022e-01,\n",
       "        4.13530e-02, -2.06657e-01,  9.29700e-02,  1.66465e-01,\n",
       "        9.08720e-02,  3.15049e-01,  2.68440e-02,  2.05318e-01,\n",
       "        2.16669e-01,  2.63040e-02,  1.68880e-02, -1.47864e-01,\n",
       "        2.61760e-02,  4.19530e-02, -2.97665e-01,  2.77722e-01,\n",
       "        6.26000e-03,  2.14811e-01,  9.43150e-02,  2.50027e-01,\n",
       "       -4.32240e-02, -1.75110e-01,  4.07981e-01, -2.33395e-01,\n",
       "       -4.79790e-02, -2.45290e-02,  8.10870e-02, -3.53514e-01,\n",
       "        4.06020e-02, -1.34088e-01,  3.39784e-01, -2.95042e-01,\n",
       "        2.57900e-03, -1.92268e-01, -1.93919e-01, -2.47287e-01,\n",
       "        2.89759e-01,  1.42659e-01, -9.98120e-02,  7.11120e-02,\n",
       "        8.00740e-02, -2.72729e-01,  1.57324e-01, -2.95570e-01,\n",
       "        4.60343e-01, -2.89715e-01,  1.58340e-01, -1.23310e-02,\n",
       "        1.67089e-01,  8.10470e-02, -4.74180e-02, -1.17763e-01,\n",
       "       -8.50440e-02, -1.42094e-01, -5.18590e-02, -6.11180e-02,\n",
       "        4.17240e-02, -1.69349e-01, -5.21960e-02,  4.71160e-02,\n",
       "        4.11223e-01, -8.72300e-02,  8.17270e-02,  1.19921e-01,\n",
       "        3.57860e-02, -2.36548e-01,  1.15918e-01,  2.51572e-01,\n",
       "        2.50406e-01,  3.55900e-03,  7.98000e-02,  7.31500e-03,\n",
       "       -1.13880e-02,  5.45800e-02,  5.39960e-02, -2.83450e-01,\n",
       "       -1.15270e-01,  3.08597e-01, -1.53684e-01, -1.85211e-01,\n",
       "       -2.70220e-02,  1.85140e-02, -1.31298e-01, -7.38920e-02,\n",
       "       -2.13500e-01,  2.21351e-01,  2.24508e-01,  1.54887e-01,\n",
       "       -5.03230e-02, -3.03589e-01,  4.39913e-01,  3.69147e-01,\n",
       "       -1.00081e-01, -2.58520e-01,  3.42676e-01, -2.37280e-02,\n",
       "        3.56900e-03, -1.87720e-02,  2.73528e-01, -6.34500e-03,\n",
       "        8.29800e-02,  5.07169e-01, -2.66802e-01,  4.33548e-01,\n",
       "       -7.51130e-02, -7.63160e-02, -2.74348e-01,  3.68409e-01,\n",
       "       -2.55348e-01, -2.69850e-01,  2.68810e-02, -1.48124e-01,\n",
       "       -2.23600e-02, -2.12513e-01, -2.08760e-01, -1.54396e-01,\n",
       "       -1.56971e-01, -7.04310e-02,  2.97522e-01, -1.65128e-01,\n",
       "       -4.50753e-01,  1.58622e-01, -1.35908e-01,  2.00874e-01,\n",
       "        2.78539e-01,  1.01763e-01,  9.58700e-03, -1.26910e-02,\n",
       "       -4.47400e-02, -1.84609e-01,  1.94337e-01,  2.96090e-02,\n",
       "        8.77990e-02,  1.92080e-01,  3.29020e-02,  1.02365e-01,\n",
       "        8.28630e-02,  2.17576e-01,  2.05978e-01, -4.92693e-01,\n",
       "       -1.84329e-01, -2.77917e-01, -8.21910e-02,  1.25196e-01,\n",
       "       -4.14922e-01, -1.99130e-02,  3.32325e-01, -8.19500e-02,\n",
       "       -2.58543e-01, -1.79119e-01, -9.20000e-05,  3.96963e-01,\n",
       "        2.17708e-01,  2.44835e-01, -6.08380e-02, -4.09320e-02,\n",
       "        1.67549e-01, -1.82423e-01,  3.06370e-02,  2.88118e-01,\n",
       "       -1.80678e-01, -1.88860e-02, -5.75218e-01, -2.36580e-01,\n",
       "       -3.29018e-01,  1.97518e-01, -1.35712e-01,  6.79500e-03,\n",
       "        2.39917e-01,  2.34290e-02,  1.28108e-01, -2.10004e-01,\n",
       "       -1.08003e-01,  3.83675e-01, -9.59050e-02,  7.38400e-02,\n",
       "       -2.60913e-01,  5.82390e-02, -2.74568e-01, -1.82261e-01,\n",
       "        3.94221e-01, -7.89790e-02, -1.46016e-01,  4.95450e-02,\n",
       "        1.48012e-01,  1.45494e-01,  1.58632e-01,  1.29716e-01,\n",
       "       -2.23840e-01,  1.92962e-01, -3.20742e-01,  2.31724e-01,\n",
       "       -1.52830e-02,  1.44979e-01, -2.38957e-01, -5.73670e-02,\n",
       "       -2.31690e-02, -1.94015e-01,  3.24610e-02, -5.29730e-02,\n",
       "        3.08373e-01,  7.25400e-02,  3.60500e-02, -1.10142e-01,\n",
       "        1.20110e-01, -1.14056e-01, -3.20005e-01,  4.71330e-02,\n",
       "        8.64740e-02,  2.29655e-01,  7.25600e-03,  2.44310e-02,\n",
       "        2.89108e-01,  2.29789e-01, -1.98163e-01, -8.93290e-02,\n",
       "       -3.52012e-01,  2.08046e-01, -4.45512e-01,  3.16600e-03,\n",
       "        3.88695e-01,  4.32900e-03,  3.06092e-01,  3.42740e-02,\n",
       "        7.80230e-02, -1.02140e-02, -1.01953e-01, -6.03020e-02,\n",
       "       -6.89800e-02,  2.55271e-01,  3.64510e-02,  2.09945e-01,\n",
       "       -9.31500e-02, -1.01780e-02, -1.76812e-01, -9.85000e-03,\n",
       "       -2.91284e-01,  3.11221e-01,  1.83861e-01,  8.53270e-02,\n",
       "       -1.36369e-01, -5.05680e-02,  1.77034e-01,  1.53194e-01,\n",
       "       -3.60576e-01, -1.53732e-01, -1.52405e-01, -1.91753e-01,\n",
       "       -8.30720e-02,  1.63690e-01, -1.48717e-01,  4.40972e-01,\n",
       "        6.81700e-02,  2.15955e-01, -1.15923e-01, -1.77890e-02,\n",
       "        2.33407e-01,  8.86470e-02, -4.74873e-01, -2.84805e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['ciano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vect_stop = TfidfVectorizer(ngram_range=(1,1), use_idf=True,stop_words=stop_words_spacy)\n",
    "vect_stop.fit(df.text_pt)\n",
    "text_vect_stop = vect_stop.transform(df.text_pt)\n",
    "\n",
    "X_train_stop,X_test_stop,y_train_stop,y_test_stop = train_test_split(\n",
    "    text_vect_stop, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 900\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df.text_pt.values)\n",
    "X = tokenizer.texts_to_sequences(df.text_pt.values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "                 \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "                 \n",
    "                 \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = [f1])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df.sentiment).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 8, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_,score_f1 = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"f1_score: %.4f\" % (score_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
